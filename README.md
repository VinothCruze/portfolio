# DATA ENGINEER

### EDUCATION
M.S in Information Technology(Specialization in Computer Hardware and Software)

### WORK EXPERIENCE
#### Data Engineer & Process Analyst @Springer Nature
•	Developed and maintained data pipelines using Apache Beam with Dataflow for efficient data loading from various data sources into Google Cloud Platform (GCP), following medallion architecture principles to BigQuery DWH.
•	Implemented observability and resiliency mechanisms using Airflow alerts, logs, and custom failure recovery scripts. Integrated Cloud Pub/Sub and Cloud Functions to manage real-time events and alerts, reducing issue detection time drastically. 
•	Reduced data quality issues by 25% via TDD implementation using dbt unit, integration testing, and SCD2 concepts with dbt snapshots to track records.
•	Designed and implemented a CI/CD pipeline using Google Cloud Build, Cloud Composer (Airflow), and Github actions, automating deployment workflows and reducing manual efforts by 30%.
•	Collaborated closely with cross-functional teams for clean data delivery, improving downstream LLM model performance used for GenAI applications.
•	Designed and developed Looker and Looker studio dashboards to address key business needs and provide actionable insights.

#### Internship Big Data Analytics und Data Engineering @BMW AG
•	Developed real-time KPI dashboards using Palantir Foundry, PySpark, and automated reports to reduce manual work by 20%.
•	Implemented NLP workflows using AWS Sagemaker, Glue, and Redshift to cluster and translate defect reports.

#### Application Development Analyst (Data Engineer & Analyst)
•	Engineered ELT pipelines in AWS using pyspark for integrating data from Oracle EBS, Salesforce, and flat files into Redshift and S3.
•	Been part of data science and BI team and helped to prepare data sets and been point of contact for any issues. Created use cases with the team for the application of machine learning possibilities.
•	Built a CapEx forecasting ML solution using Python, SQL, EMR, SageMaker and S3, which enabled early budget anomaly detection, saving ~$100K annually across global business units.
•	Led EDA phases for BI and ML projects, identifying outliers, trends, and correlations to improve data preparation. Tuned performance of SQL/PLSQL procedures, improving query execution by 20%.
•	Managed delivery, ensured functionality validation, data validation, and robust data pipelines with alerts of success and failure with reasons from time to time.
•	Managed Hadoop ingestion via SQOOP, Hive scripting for optimized ORC storage, and worked across multiple data formats (JSON, CSV, Avro, Parquet).

### PROJECTS
#### Sparkify data lake using AWS
•	Built a data lake for a fictional online data streaming company called Sparkify. Collected the data using API as JSON data with user and metadata of song details and stored the data in HDFS parquet files on AWS S3 bucket using AWS EMR cluster and used Infrastructure as Code applicability for automatic creation of EMR clusters and IAM roles for job automation.
#### Snowflake ETL workflow:
•	Customer sales data with customer, order and item details dataset are captured in on-prem RDBMS which was moved to AWS S3 bucket as external staging with the SNS service, and then the data was loaded into landing zone with snowpipe and then with task and task tree, moved data into curated zone and captured the changes and finally loaded into the consumption layer for further usage, made it to be fully automated pipeline

### TECHNICAL SKILLS
 
•	#### Programming & Tools: Python, Pydantic, PySpark, SQL, JavaScript, Flask, Shell, GitHub Actions
•	#### Data Engineering: Apache Beam, Apache Airflow (Cloud Composer), dbt, Dataflow, RAG, vector database, Redshift, S3, EMR, Glue, Snowflake
•	#### CI/CD & Infrastructure: GitHub Actions, GCP Cloud Build, Terraform, Docker, REST APIs
•	#### Cloud Platforms: Google Cloud Platform (GCP), AWS, Palantir Foundry
•	#### Databases: Oracle 10g/11g/12c, PostgreSQL, MySQL, Sybase, NOSQL Database - Cassandra
•	#### Testing & Monitoring: Test-Driven Development (TDD), Observability, Monitoring, Logging
•	#### Visualization & Reporting: Looker, Power BI, d3.js, Celonis PQL
 
 
### CERTIFICATIONS

•	Databricks Certified Data Engineer Associate (Databricks)
•	Microsoft Azure for Data Engineering (Coursera)

 
### HOBBIES

•	Cycling, Badminton
 

### LANGUAGES
•	English (Full Professional Proficiency (CEFR C1)
•	German (Limited working proficiency – B1.1)
•	Tamil (Native language)


